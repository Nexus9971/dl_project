{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 19:36:17.040593: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-22 19:36:17.080741: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-22 19:36:17.760954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#Math Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Metrics Libraries\n",
    "from torchmetrics import Metric\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchmetrics.regression import MeanAbsoluteError\n",
    "\n",
    "#Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#Graphic Libraries\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#System Libraries\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.mem_get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 100\n",
    "LABELS = 7\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, root=\"./cityscapes_preprocessed\", split=\"train\", labels=7):\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.images = glob.glob(os.path.join(root, split, \"image\", \"*.npy\"))\n",
    "        self.labels = glob.glob(os.path.join(root, split, f\"label_{labels}\", \"*.npy\"))\n",
    "        self.depth = glob.glob(os.path.join(root, split, \"depth\", \"*.npy\"))\n",
    "        self.images.sort()\n",
    "        self.labels.sort()\n",
    "        self.depth.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.from_numpy(np.load(self.images[idx])).permute(2, 0, 1)\n",
    "        label = torch.from_numpy(np.load(self.labels[idx]))\n",
    "        depth = torch.from_numpy(np.load(self.depth[idx])).squeeze(2)\n",
    "        return image, label, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_train = CityscapesDataset(split=\"train\",labels=LABELS)\n",
    "train_dl = DataLoader(cityscapes_train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "for image, label, depth in train_dl:\n",
    "    print(f'Image: {image.shape}, Label: {label.shape}, Depth: {depth.shape}')\n",
    "    print(f'Image: {image.max()}, {image.min()}') \n",
    "    print(f'Label: {label.max()}, {label.min()}')\n",
    "    print(f'Depth: {depth.max()}, {depth.min()}')\n",
    "    break\n",
    "ax, fig = plt.subplots(3, figsize=(10, 10))\n",
    "fig[0].imshow(image[0].permute(1, 2, 0))\n",
    "fig[1].imshow(label[0])\n",
    "fig[2].imshow(depth[0])\n",
    "plt.show()\n",
    "\n",
    "cityscapes_val = CityscapesDataset(split=\"val\", labels=LABELS)\n",
    "val_dl = DataLoader(cityscapes_val, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.conv(x)\n",
    "        logits = self.bn(logits)\n",
    "        logits = self.relu(logits)\n",
    "        return logits\n",
    "\n",
    "class DownSampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_layer = ConvLayer(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer(x)\n",
    "        logits, indices = self.pool(out)\n",
    "        return logits, indices, out\n",
    "\n",
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.conv_layer = ConvLayer(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, indices):\n",
    "        logits = self.unpool(x, indices)\n",
    "        up_layer = logits\n",
    "        logits = self.conv_layer(logits)\n",
    "        return logits, up_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTAN model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderSH(nn.Module):\n",
    "    def __init__(self, filter):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList()\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.enc_blocks.append(ConvLayer(3, filter[0]))\n",
    "        self.down_blocks.append(DownSampleBlock(filter[0], filter[0]))\n",
    "        for i in range(len(filter) - 1):\n",
    "            self.enc_blocks.append(ConvLayer(filter[i], filter[i+1]))\n",
    "            self.down_blocks.append(DownSampleBlock(filter[i+1], filter[i+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_indices = []\n",
    "        down_layer = []\n",
    "        enc_layer = []\n",
    "        out = []\n",
    "        logits = x\n",
    "        for i in range(len(self.down_blocks)):\n",
    "            logits = self.enc_blocks[i](logits)\n",
    "            enc_layer.append(logits)\n",
    "            logits, indices, down = self.down_blocks[i](logits)\n",
    "            down_layer.append(down)    \n",
    "            out.append(logits)\n",
    "            down_indices.append(indices)\n",
    "            \n",
    "        return logits, enc_layer, down_layer, down_indices, out\n",
    "\n",
    "class DecoderSH(nn.Module):\n",
    "    def __init__(self, filter):\n",
    "        super().__init__()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        self.dec_blocks = nn.ModuleList()\n",
    "        for i in range(len(filter) - 1):\n",
    "            self.up_blocks.append(UpSampleBlock(filter[i], filter[i+1]))\n",
    "            self.dec_blocks.append(ConvLayer(filter[i+1], filter[i+1]))\n",
    "        self.up_blocks.append(UpSampleBlock(filter[-1], filter[-1]))\n",
    "        self.dec_blocks.append(ConvLayer(filter[-1], filter[-1]))\n",
    "            \n",
    "\n",
    "    def forward(self, x, down_indices):\n",
    "        up_layer = []\n",
    "        dec_layer = []\n",
    "        logits = x\n",
    "        for i in range(len(self.up_blocks)):\n",
    "            logits, up = self.up_blocks[i](logits, down_indices[-(i+1)])\n",
    "            up_layer.append(up)\n",
    "            logits = self.dec_blocks[i](logits)\n",
    "            dec_layer.append(logits)\n",
    "        return up_layer, dec_layer\n",
    "    \n",
    "class SharedNet(nn.Module):\n",
    "    def __init__(self, filter):\n",
    "        super().__init__()\n",
    "        self.enc = EncoderSH(filter)\n",
    "        self.dec = DecoderSH([filter[-(i+1)] for i in range(len(filter))])  \n",
    "\n",
    "    def forward(self, x):\n",
    "        logits, enc_layer, down_layer, down_indices, enc_out = self.enc(x)\n",
    "        enc_dict = {'out': enc_layer, 'down': down_layer}\n",
    "        up_layer, dec_layer = self.dec(logits, down_indices)\n",
    "        dec_dict = {'out': dec_layer, 'up': up_layer}\n",
    "        out_dict = {'enc': enc_out, 'dec': dec_layer}\n",
    "        return enc_dict, dec_dict, down_indices, out_dict\n",
    "    \n",
    "class AttEncBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.att_layer_g = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.att_layer_h = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels, mid_channels, kernel_size=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att_layer_f = ConvLayer(mid_channels, out_channels)\n",
    "        self.down = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, enc_layer, down_layer, x=None):\n",
    "        logits = enc_layer if x == None else torch.cat([enc_layer, x], dim=1)\n",
    "        g = self.att_layer_g(logits)\n",
    "        h = self.att_layer_h(g)\n",
    "        p = h * down_layer\n",
    "        logits = self.att_layer_f(p)\n",
    "        logits = self.down(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class AttDecBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2)\n",
    "        self.att_layer_f = ConvLayer(in_channels, out_channels)\n",
    "        self.att_layer_g = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.att_layer_h = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, up_layer, dec_layer):\n",
    "        logits = self.up(x)\n",
    "        logits = self.att_layer_f(logits)\n",
    "        logits = torch.cat([logits, up_layer], dim=1)\n",
    "        g = self.att_layer_g(logits)\n",
    "        h = self.att_layer_h(g)\n",
    "        p = h * dec_layer\n",
    "        logits = p\n",
    "        return logits\n",
    "    \n",
    "class AttNet(nn.Module):\n",
    "    def __init__(self, filter):\n",
    "        super().__init__()\n",
    "        self.enc_att = nn.ModuleList()\n",
    "        self.dec_att = nn.ModuleList()\n",
    "\n",
    "        self.enc_att.append(AttEncBlock(filter[0], filter[0], filter[1]))\n",
    "        for i in range(1,len(filter)-1):\n",
    "            self.enc_att.append(AttEncBlock(2*filter[i], filter[i], filter[i+1]))\n",
    "        self.enc_att.append(AttEncBlock(2*filter[-1], filter[-1], filter[-1]))\n",
    "        \n",
    "        for i in range(1, len(filter)):\n",
    "            self.dec_att.append(AttDecBlock(filter[-i], filter[-i]+filter[-i-1], filter[-i-1]))\n",
    "        self.dec_att.append(AttDecBlock(filter[0], 2*filter[0], filter[0]))\n",
    "\n",
    "    def forward(self, enc_dict, dec_dict):\n",
    "        for i in range(len(self.enc_att)):\n",
    "            if i == 0:\n",
    "                logits = self.enc_att[i](enc_dict['out'][i], enc_dict['down'][i])\n",
    "            else:\n",
    "                logits = self.enc_att[i](enc_dict['out'][i], enc_dict['down'][i], logits)\n",
    "\n",
    "        for i in range(len(self.dec_att)):\n",
    "            logits = self.dec_att[i](logits, dec_dict['up'][i], dec_dict['out'][i])\n",
    "        return logits\n",
    "    \n",
    "class MTAN(nn.Module):\n",
    "    def __init__(self, classes=7, tasks=2):\n",
    "        super().__init__()\n",
    "        self.name = \"mtan\"\n",
    "        filter = [64, 128, 256, 512, 512]\n",
    "        self.classes = classes + 1 #background\n",
    "        self.tasks = tasks\n",
    "        self.sh_net = SharedNet(filter)\n",
    "        self.attnet_task = nn.ModuleList([AttNet(filter) for _ in range(tasks)])\n",
    "        #to train with cross entropy loss\n",
    "        self.seg_head = nn.Conv2d(filter[0], self.classes, kernel_size=1)\n",
    "        #to train with L1 loss\n",
    "        self.depth_head = nn.Sequential(\n",
    "            nn.Conv2d(filter[0], 1, kernel_size=1), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_dict, dec_dict, _, _ = self.sh_net(x)\n",
    "        logits = []\n",
    "        for i in range(self.tasks):\n",
    "            logits.append(self.attnet_task[i](enc_dict, dec_dict))\n",
    "        logits_seg = self.seg_head(logits[0])\n",
    "        logits_depth = self.depth_head(logits[1])\n",
    "        return logits_seg, logits_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskNet(nn.Module):\n",
    "    def __init__(self, filter, classes):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.start_conv = nn.Sequential(\n",
    "            ConvLayer(3, filter[0]),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )  \n",
    "        self.dense_enc = nn.ModuleList()\n",
    "        self.dense_dec = nn.ModuleList()\n",
    "        for i in range(len(filter)-1):\n",
    "            dense_block_enc = nn.Sequential(\n",
    "                ConvLayer(2*filter[i], filter[i+1]),\n",
    "                ConvLayer(filter[i+1], filter[i+1]),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)   \n",
    "            )\n",
    "            self.dense_enc.append(dense_block_enc)\n",
    "        dense_block_enc = nn.Sequential(\n",
    "            ConvLayer(2*filter[-1], filter[-1]),\n",
    "            ConvLayer(filter[-1], filter[-1]),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        self.dense_enc.append(dense_block_enc)\n",
    "        for i in range(len(filter)-1):\n",
    "            dense_block_dec = nn.Sequential(\n",
    "                ConvLayer(filter[-i-1]+filter[-i-2], filter[-i-2]),\n",
    "                ConvLayer(filter[-i-2], filter[-i-2]),\n",
    "                nn.Upsample(scale_factor=2)\n",
    "            )\n",
    "            self.dense_dec.append(dense_block_dec)\n",
    "        dense_block_dec = nn.Sequential(\n",
    "            ConvLayer(filter[0]+filter[0], filter[0]),\n",
    "            ConvLayer(filter[0], filter[0])\n",
    "        )\n",
    "        self.dense_dec.append(dense_block_dec)\n",
    "        self.head = nn.Sequential(\n",
    "            ConvLayer(filter[0], filter[0]),\n",
    "            ConvLayer(filter[0], filter[0]),\n",
    "            nn.Conv2d(filter[0], classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, out_dict):\n",
    "        logits = self.start_conv(x)\n",
    "        for i in range(len(self.dense_enc)):\n",
    "            feat_in = torch.cat((logits, out_dict['enc'][i]), dim=1)\n",
    "            logits = self.dense_enc[i](feat_in)\n",
    "        for i in range(len(self.dense_dec)):\n",
    "            feat_in = torch.cat((logits, out_dict['dec'][i]), dim=1)\n",
    "            logits = self.dense_dec[i](feat_in)\n",
    "        logits = self.head(logits)\n",
    "        return logits\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, classes=7):\n",
    "        super().__init__()\n",
    "        self.name = \"densenet\"\n",
    "        filter = [64, 128, 256, 512, 512]\n",
    "        self.classes = classes\n",
    "        self.sh_net = SharedNet(filter)\n",
    "        self.seg_net = TaskNet(filter, self.classes+1)\n",
    "        self.depth_net = TaskNet(filter, classes=1)\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, _, out_dict = self.sh_net(x)\n",
    "        logits_seg = self.seg_net(x, out_dict)\n",
    "        logits_depth = self.depth_net(x, out_dict)\n",
    "        return logits_seg, logits_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegNet, DepthNet, SplitNet model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, filter):\n",
    "        super().__init__()\n",
    "        start_block = nn.Sequential(\n",
    "            ConvLayer(3, filter[0]), \n",
    "            ConvLayer(filter[0], filter[0]), \n",
    "            ConvLayer(filter[0], filter[0])\n",
    "        )\n",
    "        self.enc_blocks = nn.ModuleList([start_block])\n",
    "        self.down_blocks = nn.ModuleList([DownSampleBlock(filter[0], filter[0])])\n",
    "        for i in range(len(filter) - 1):\n",
    "            block = nn.Sequential(\n",
    "                ConvLayer(filter[i], filter[i+1]), \n",
    "                ConvLayer(filter[i+1], filter[i+1]), \n",
    "                ConvLayer(filter[i+1], filter[i+1]),\n",
    "            )\n",
    "            self.enc_blocks.append(block)\n",
    "            self.down_blocks.append(DownSampleBlock(filter[i+1], filter[i+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_indices = []\n",
    "        logits = x\n",
    "        for i in range(len(self.down_blocks)):\n",
    "            logits = self.enc_blocks[i](logits)\n",
    "            logits, indices, _ = self.down_blocks[i](logits)\n",
    "            down_indices.append(indices)\n",
    "        return logits, down_indices\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, filter):\n",
    "        super().__init__()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        self.dec_blocks = nn.ModuleList()\n",
    "        for i in range(len(filter) - 1):\n",
    "            block = nn.Sequential(\n",
    "                ConvLayer(filter[i+1], filter[i+1]), \n",
    "                ConvLayer(filter[i+1], filter[i+1]),\n",
    "                ConvLayer(filter[i+1], filter[i+1])\n",
    "            )\n",
    "            self.dec_blocks.append(block)\n",
    "            self.up_blocks.append(UpSampleBlock(filter[i], filter[i+1]))\n",
    "        self.up_blocks.append(UpSampleBlock(filter[-1], filter[-1]))\n",
    "        block = nn.Sequential(\n",
    "            ConvLayer(filter[-1], filter[-1]), \n",
    "            ConvLayer(filter[-1], filter[-1]),\n",
    "            ConvLayer(filter[-1], filter[-1])\n",
    "        )\n",
    "        self.dec_blocks.append(block)\n",
    "\n",
    "    def forward(self, x, down_indices):\n",
    "        logits = x\n",
    "        for i in range(len(self.up_blocks)):\n",
    "            logits, _ = self.up_blocks[i](logits, down_indices[-(i+1)])\n",
    "            logits = self.dec_blocks[i](logits)\n",
    "        return logits\n",
    "\n",
    "class SegNet(nn.Module):\n",
    "    def __init__(self, classes=7, mid_layers=4):\n",
    "        super().__init__()\n",
    "        self.name = \"segnet\"\n",
    "        filter = [64, 128, 256, 512, 512]\n",
    "        self.classes = classes + 1\n",
    "        self.enc_net = Encoder(filter)\n",
    "        self.mid_net = nn.Sequential(*[ConvLayer(filter[-1], filter[-1]) for _ in range(mid_layers)])\n",
    "        self.dec_net = Decoder([filter[-(i+1)] for i in range(len(filter))])\n",
    "        \n",
    "        self.seg_head = nn.Sequential(\n",
    "            ConvLayer(filter[0], filter[0]),\n",
    "            nn.Conv2d(filter[0], self.classes, kernel_size=1)\n",
    "        )\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits, down_indices = self.enc_net(x)\n",
    "        logits = self.mid_net(logits)\n",
    "        logits = self.dec_net(logits, down_indices)\n",
    "        logits = self.seg_head(logits)\n",
    "        return logits\n",
    "\n",
    "class DepthNet(nn.Module):\n",
    "    def __init__(self, mid_layers=4):\n",
    "        super().__init__()\n",
    "        filter = [64, 128, 256, 512, 512]\n",
    "        self.name = \"depthnet\"\n",
    "        self.classes = 1\n",
    "        self.enc_net = Encoder(filter)\n",
    "        self.mid_net = nn.Sequential(*[ConvLayer(filter[-1], filter[-1]) for _ in range(mid_layers)])\n",
    "        self.dec_net = Decoder([filter[-(i+1)] for i in range(len(filter))])\n",
    "        self.depth_head = nn.Sequential(\n",
    "            ConvLayer(filter[0], filter[0]),\n",
    "            nn.Conv2d(filter[0], self.classes, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits, down_indices = self.enc_net(x)\n",
    "        logits = self.mid_net(logits)\n",
    "        logits = self.dec_net(logits, down_indices)\n",
    "        logits = self.depth_head(logits)\n",
    "        return logits\n",
    "\n",
    "class SplitNet(nn.Module):\n",
    "    def __init__(self, classes=7):\n",
    "        super().__init__()\n",
    "        self.name = \"splitnet\"\n",
    "        filter = [64, 128, 256, 512, 512]\n",
    "        self.classes = classes + 1\n",
    "        self.enc_net = Encoder(filter)\n",
    "        self.mid_net = nn.Sequential(\n",
    "            ConvLayer(filter[-1], filter[-1]),\n",
    "            ConvLayer(filter[-1], filter[-1]),\n",
    "            ConvLayer(filter[-1], filter[-1]),\n",
    "            ConvLayer(filter[-1], filter[-1]),\n",
    "        )\n",
    "        self.dec_net = Decoder([filter[-(i+1)] for i in range(len(filter))])\n",
    "        self.seg_head = nn.Sequential(\n",
    "            ConvLayer(filter[0], filter[0]),\n",
    "            ConvLayer(filter[0], filter[0]),\n",
    "            nn.Conv2d(filter[0], self.classes, kernel_size=1)\n",
    "        )\n",
    "        self.depth_head = nn.Sequential(\n",
    "            ConvLayer(filter[0], filter[0]),\n",
    "            ConvLayer(filter[0], filter[0]),\n",
    "            nn.Conv2d(filter[0], 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits, down_indices = self.enc_net(x)\n",
    "        logits = self.mid_net(logits)\n",
    "        logits = self.dec_net(logits, down_indices)\n",
    "        logits_seg = self.seg_head(logits)\n",
    "        logits_depth = self.depth_head(logits)\n",
    "        return logits_seg, logits_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossStitch model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossStitchNet(nn.Module):\n",
    "    def __init__(self, classes=7, tasks=2):\n",
    "        super().__init__()\n",
    "        self.name = \"crossstitch\"\n",
    "        filter = [64, 128, 256, 512, 512]\n",
    "        self.classes = classes + 1\n",
    "        self.tasks = tasks\n",
    "        self.alphas = nn.ParameterList([nn.Parameter(torch.rand(2)) for _ in range(2)])\n",
    "\n",
    "        self.nets = nn.ModuleList()\n",
    "        self.nets.append(nn.ModuleList())\n",
    "        self.nets.append(nn.ModuleList())\n",
    "\n",
    "        for i in range(tasks):\n",
    "            self.nets[i].append(ConvLayer(3, filter[0]))\n",
    "        # self.netA = nn.ModuleList()\n",
    "        # self.netB = nn.ModuleList()\n",
    "        # self.netA.append(ConvLayer(3, filter[0]))\n",
    "        # self.netB.append(ConvLayer(3, filter[0]))\n",
    "\n",
    "        for i in range(len(filter)-1):\n",
    "            for j in range(tasks):\n",
    "                self.nets[j].append(ConvLayer(filter[i], filter[i+1]))\n",
    "                self.nets[j].append(ConvLayer(filter[i+1], filter[i+1]))\n",
    "                self.nets[j].append(DownSampleBlock(filter[i+1], filter[i+1]))\n",
    "\n",
    "            # self.netA.append(ConvLayer(filter[i], filter[i+1]))\n",
    "            # self.netA.append(ConvLayer(filter[i+1], filter[i+1]))\n",
    "            # self.netB.append(ConvLayer(filter[i], filter[i+1]))\n",
    "            # self.netB.append(ConvLayer(filter[i+1], filter[i+1]))\n",
    "            #for _ in range(2):\n",
    "                #self.netA.append(ConvLayer(filter[i+1], filter[i+1]))\n",
    "                #self.netB.append(ConvLayer(filter[i+1], filter[i+1]))\n",
    "            # self.netA.append(DownSampleBlock(filter[i+1], filter[i+1]))\n",
    "            # self.netB.append(DownSampleBlock(filter[i+1], filter[i+1]))\n",
    "\n",
    "        # for _ in range(2):\n",
    "        #     self.netA.append(ConvLayer(filter[-1], filter[-1]))\n",
    "        #     self.netB.append(ConvLayer(filter[-1], filter[-1]))\n",
    "\n",
    "        for i in range(len(filter)-1):\n",
    "            for j in range(tasks):\n",
    "                self.nets[j].append(UpSampleBlock(filter[-(i+1)], filter[-(i+2)]))\n",
    "                self.nets[j].append(ConvLayer(filter[-(i+2)], filter[-(i+2)]))\n",
    "                self.nets[j].append(ConvLayer(filter[-(i+2)], filter[-(i+2)]))\n",
    "            # self.netA.append(UpSampleBlock(filter[-(i+1)], filter[-(i+2)]))\n",
    "            # self.netB.append(UpSampleBlock(filter[-(i+1)], filter[-(i+2)]))\n",
    "            #for _ in range(2):\n",
    "                #self.netA.append(ConvLayer(filter[-(i+2)], filter[-(i+2)]))\n",
    "                #self.netB.append(ConvLayer(filter[-(i+2)], filter[-(i+2)]))\n",
    "            # self.netA.append(ConvLayer(filter[-(i+2)], filter[-(i+2)]))\n",
    "            # self.netA.append(ConvLayer(filter[-(i+2)], filter[-(i+2)]))\n",
    "            # self.netB.append(ConvLayer(filter[-(i+2)], filter[-(i+2)]))\n",
    "            # self.netB.append(ConvLayer(filter[-(i+2)], filter[-(i+2)]))\n",
    "\n",
    "        heads = nn.ModuleList()\n",
    "        heads.append(nn.Conv2d(filter[0], self.classes, kernel_size=1))\n",
    "        heads.append(nn.Sequential(\n",
    "            nn.Conv2d(filter[0], 1, kernel_size=1), \n",
    "            nn.Sigmoid())\n",
    "        )\n",
    "        for i in range(tasks):\n",
    "            self.nets[i].append(ConvLayer(filter[0], filter[0]))\n",
    "            self.nets[i].append(heads[i])\n",
    "        # self.nets[0].append(nn.Conv2d(filter[0], self.classes, kernel_size=1))\n",
    "        # self.nets[1].append(nn.Conv2d(filter[0], 1, kernel_size=1))\n",
    "        # self.nets[1].append(nn.Sigmoid())\n",
    "        # self.netA.append(ConvLayer(filter[0], filter[0]))\n",
    "        # self.netA.append(nn.Conv2d(filter[0], self.classes, kernel_size=1))\n",
    "        # self.netB.append(ConvLayer(filter[0], filter[0]))\n",
    "        # self.netB.append(nn.Conv2d(filter[0], 1, kernel_size=1))\n",
    "        # self.netB.append(nn.Sigmoid())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits_seg = x \n",
    "        logits_depth = x\n",
    "        indices_A = []\n",
    "        indices_B = []\n",
    "        j = 1\n",
    "        for modA, modB in zip(self.nets[0], self.nets[1]):\n",
    "            if isinstance(modA, DownSampleBlock):\n",
    "                logits_seg, idx_A, _ = modA(logits_seg)\n",
    "                logits_depth, idx_B, _ = modB(logits_depth)\n",
    "                indices_A.append(idx_A)\n",
    "                indices_B.append(idx_B)\n",
    "                logits_seg = self.alphas[0][0] * logits_seg + self.alphas[0][1] * logits_depth\n",
    "                logits_depth = self.alphas[1][0] * logits_depth + self.alphas[1][1] * logits_seg\n",
    "            elif isinstance(modA, UpSampleBlock):\n",
    "                logits_seg, _ = modA(logits_seg, indices_A[-j])\n",
    "                logits_depth, _ = modB(logits_depth, indices_B[-j])\n",
    "                j += 1\n",
    "                logits_seg = self.alphas[0][0] * logits_seg + self.alphas[0][1] * logits_depth\n",
    "                logits_depth = self.alphas[1][0] * logits_depth + self.alphas[1][1] * logits_seg\n",
    "            else: #isinstance(modA, ConvLayer) or head\n",
    "                logits_seg = modA(logits_seg)\n",
    "                logits_depth = modB(logits_depth)\n",
    "        return logits_seg, logits_depth\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segnet = SegNet()\n",
    "depthnet = DepthNet()\n",
    "mtan = MTAN()\n",
    "splitnet = SplitNet()\n",
    "crossstitch = CrossStitchNet()\n",
    "densenet = DenseNet()\n",
    "print(f\"SegNet parameters: {count_params(segnet)}\")\n",
    "print(f\"DepthNet parameters: {count_params(depthnet)}\")\n",
    "\n",
    "print(f\"MTAN parameters: {count_params(mtan)}\")\n",
    "print(f\"DenseNet parameters: {count_params(densenet)}\")\n",
    "print(f\"CrossStitchNet parameters: {count_params(crossstitch)}\")\n",
    "print(f\"SplitNet parameters: {count_params(splitnet)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAbsoluteRelativeError(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"sum_rel_err\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"num_obs\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        sum_abs_target = torch.sum(torch.abs(target))\n",
    "        self.sum_rel_err += torch.sum(torch.abs(preds - target))/sum_abs_target\n",
    "        self.num_obs += target.shape[0]\n",
    "\n",
    "    def compute(self):\n",
    "        return self.sum_rel_err / self.num_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_plt(plt, data):\n",
    "    for k in data.keys():\n",
    "        plt[k].append(data[k].compute().cpu()) if isinstance(data[k], Metric) else plt[k].append(data[k])\n",
    "\n",
    "def compute_lambdas(losses_seg, losses_depth, T, K):\n",
    "    w_seg = np.mean(losses_seg['new']) / np.mean(losses_seg['old'])\n",
    "    w_depth = np.mean(losses_depth['new']) / np.mean(losses_depth['old'])\n",
    "    w = F.softmax(torch.tensor([w_seg/T, w_depth/T]), dim=0)*K\n",
    "    return w\n",
    "\n",
    "def update_stats(stats, x, y):\n",
    "    for k in stats.keys():\n",
    "        stats[k].update(x, y)\n",
    "\n",
    "def reset_stats(stats):\n",
    "    for k in stats.keys():\n",
    "        stats[k].reset()\n",
    "\n",
    "def update_losses(losses_seg, losses_depth):\n",
    "    losses_seg['old'] = losses_seg['new']\n",
    "    losses_depth['old'] = losses_depth['new']\n",
    "    losses_seg['new'] = []\n",
    "    losses_depth['new'] = []\n",
    "\n",
    "# def save_fig_plots(model, epochs, plots=None):\n",
    "#     plt.savefig(f\"./models/{model.name}/{model.name}_train{epochs}.png\")\n",
    "#     torch.save(model.state_dict(), f\"./models/{model.name}/{model.name}_train{epochs}.pth\")\n",
    "#     if plots is not None:\n",
    "#         if not os.path.exists(f\"./models/{model.name}/plots\"):\n",
    "#             os.makedirs(f\"./models/{model.name}/plots\")\n",
    "#         for k in plots.keys():\n",
    "#             torch.save(plots[k], f\"./models/{model.name}/plots/{k}.pth\")\n",
    "\n",
    "def save_model_opt(model, opt, epochs):\n",
    "    torch.save(model.state_dict(), f\"./models/{model.name}/{model.name}_train{epochs}.pth\")\n",
    "    torch.save(opt.state_dict(), f\"./models/{model.name}/{model.name}_opt_train{epochs}.pth\")\n",
    "\n",
    "def compute_loss_multitask(model, x, y_seg, y_dis, stats_seg, stats_depth):\n",
    "    x = x.to(DEVICE).to(torch.float)\n",
    "    y_seg = y_seg.to(DEVICE).to(torch.long)\n",
    "    y_dis = y_dis.to(DEVICE).to(torch.float)\n",
    "    loss_fn_seg = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "    loss_fn_depth = nn.L1Loss()\n",
    "    output_seg, output_depth = model(x)\n",
    "    \n",
    "    loss_seg = loss_fn_seg(output_seg, y_seg)\n",
    "    loss_depth = loss_fn_depth(output_depth.squeeze(1), y_dis)\n",
    "\n",
    "    preds_seg = torch.argmax(output_seg, dim=1)\n",
    "    preds_seg_flat = preds_seg.view(-1)\n",
    "    y_seg_flat = y_seg.view(-1)\n",
    "    pos_idx = torch.where(y_seg_flat != -1)\n",
    "    preds_seg_flat = preds_seg_flat[pos_idx[0]].unsqueeze(0)\n",
    "    y_seg_flat = y_seg_flat[pos_idx[0]].unsqueeze(0)\n",
    "\n",
    "    update_stats(stats_seg, preds_seg_flat, y_seg_flat)\n",
    "    update_stats(stats_depth, output_depth.squeeze(1), y_dis)\n",
    "    return loss_seg, loss_depth    \n",
    "\n",
    "def compute_grad(model):\n",
    "    params = [p for p in model.parameters() if p.grad is not None and p.requires_grad]\n",
    "    grad_norm = 0\n",
    "    for p in params:\n",
    "        p_grad = p.grad.detach().data.norm(2).item()\n",
    "        grad_norm += p_grad**2\n",
    "    return grad_norm**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask_dwa(model, opt, train_dl, val_dl=None, epochs=10, update_lambdas=10, T=2, save=False, check=5, grad=False):\n",
    "    model = model.to(DEVICE)\n",
    "    writer = SummaryWriter(f'./runs/{model.name}')\n",
    "\n",
    "    lambdas = np.array([1, 1])\n",
    "    losses_seg = {'new': [], 'old': []}\n",
    "    losses_depth = {'new': [], 'old': []}\n",
    "    plt_losses_train = {'seg': [], 'depth': [], 'total': []}\n",
    "    plt_stats_train = {'miou': [], 'pix_acc': [], 'mae': [], 'mre': []}\n",
    "    plt_lambdas = {'lambda0': [], 'lambda1': []}\n",
    "    plt_grad = []\n",
    "\n",
    "    miou = MeanIoU(num_classes=model.classes, per_class=False, include_background=False, input_format='index').to(DEVICE)\n",
    "    pix_acc = MulticlassAccuracy(num_classes=model.classes, multidim_average='global', average='micro').to(DEVICE)\n",
    "    stats_seg = {'miou':miou, 'pix_acc':pix_acc}\n",
    "    mae = MeanAbsoluteError().to(DEVICE)\n",
    "    mre = MeanAbsoluteRelativeError().to(DEVICE)\n",
    "    stats_depth = {'mae':mae, 'mre':mre}\n",
    "    if val_dl != None:\n",
    "        plt_losses_val = {'seg': [], 'depth': [], 'total': []}\n",
    "        plt_stats_val = {'miou': [], 'pix_acc': [], 'mae': [], 'mre': []}\n",
    "        \n",
    "    if save and not os.path.exists(f\"./models/{model.name}\"): \n",
    "        os.makedirs(f\"./models/{model.name}\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        reset_stats(stats_seg)\n",
    "        reset_stats(stats_depth)\n",
    "    \n",
    "        total_loss = 0\n",
    "        total_loss_seg = 0\n",
    "        total_loss_depth = 0\n",
    "        for x, y_seg, y_dis in tqdm(train_dl):\n",
    "            opt.zero_grad()\n",
    "            loss_seg, loss_depth = compute_loss_multitask(model, x, y_seg, y_dis, stats_seg, stats_depth)\n",
    "            loss = lambdas[0]*loss_seg + lambdas[1]*loss_depth\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            losses_seg['new'].append(loss_seg.item())\n",
    "            losses_depth['new'].append(loss_depth.item())\n",
    "            if len(losses_seg['new']) == 2*update_lambdas and len(losses_seg['old']) == 0:\n",
    "                losses_seg['old'] = losses_seg['new'][0:update_lambdas]\n",
    "                losses_depth['old'] = losses_depth['new'][0:update_lambdas]\n",
    "                losses_seg['new'] = losses_seg['new'][update_lambdas:]\n",
    "                losses_depth['new'] = losses_depth['new'][update_lambdas:]\n",
    "\n",
    "                lambdas = compute_lambdas(losses_seg, losses_depth, T, model.classes)\n",
    "                update_losses(losses_seg, losses_depth)\n",
    "\n",
    "            if len(losses_seg['new']) == update_lambdas and len(losses_seg['old']) == update_lambdas:\n",
    "                lambdas = compute_lambdas(losses_seg, losses_depth, T, model.classes)\n",
    "                update_losses(losses_seg, losses_depth)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_loss_seg += loss_seg.item()\n",
    "            total_loss_depth += loss_depth.item()\n",
    "\n",
    "        plt_lambdas['lambda0'].append(lambdas[0].item())\n",
    "        plt_lambdas['lambda1'].append(lambdas[1].item())\n",
    "        total_loss /= len(train_dl)\n",
    "        total_loss_seg /= len(train_dl)\n",
    "        total_loss_depth /= len(train_dl)\n",
    "        plt_losses_train['seg'].append(total_loss_seg)\n",
    "        plt_losses_train['depth'].append(total_loss_depth)\n",
    "        plt_losses_train['total'].append(total_loss)\n",
    "        # add_plt(plt_stats_train, stats_seg)\n",
    "        # add_plt(plt_stats_train, stats_depth)\n",
    "        print_stats = dict(stats_seg, **stats_depth)\n",
    "        add_plt(plt_stats_train, print_stats)\n",
    "        if grad:\n",
    "            grad_norm = compute_grad(model)\n",
    "            plt_grad.append(grad_norm)\n",
    "            writer.add_scalar('Train/Gradient', grad_norm, epoch) \n",
    "        if epoch % check == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} - Train Total Loss: {total_loss:.4f}\")\n",
    "            print(f\"Lambda_0: {lambdas[0]} - Train Loss Segmentation: {total_loss_seg:.4f}\")\n",
    "            print(f\"Lambda_1: {lambdas[1]} - Train Loss Depth: {total_loss_depth:.4f}\")\n",
    "            for k in print_stats.keys():\n",
    "                print(f\"{k}: {print_stats[k].compute().cpu()}\")\n",
    "            print(f\"Gradient Norm: {grad_norm}\\n\") if grad else print(\"\\n\")\n",
    "            save_model_opt(model, opt, epoch) if save else None\n",
    "        writer.add_scalar('Train/Loss/Total', total_loss, epoch)\n",
    "        writer.add_scalar('Train/Loss/Segmentation', total_loss_seg, epoch)\n",
    "        writer.add_scalar('Train/Loss/Depth', total_loss_depth, epoch)\n",
    "        for k in print_stats.keys():\n",
    "            writer.add_scalar(f'Train/{k}', print_stats[k].compute().cpu(), epoch)\n",
    "                \n",
    "        if val_dl != None and epoch % check == 0:\n",
    "            losses_tmp, stats_tmp = val_epoch_multitask(model, val_dl, writer, epoch)\n",
    "            add_plt(plt_losses_val, losses_tmp)\n",
    "            add_plt(plt_stats_val, stats_tmp)\n",
    "\n",
    "    _, ax = plt.subplots(4, 2, figsize=(40, 40)) if not grad else plt.subplots(5, 2, figsize=(50, 50))\n",
    "    ax[0][0].plot(plt_losses_train['seg'])\n",
    "    ax[0][0].set_title('Segmentation Loss')\n",
    "    ax[0][1].plot(plt_losses_train['depth'])\n",
    "    ax[0][1].set_title('Depth Loss')\n",
    "    ax[1][0].plot(plt_lambdas['lambda0'])\n",
    "    ax[1][0].plot(plt_lambdas['lambda1'])\n",
    "    ax[1][0].set_title('Lambdas')\n",
    "    ax[1][1].plot(plt_losses_train['total'])\n",
    "    ax[1][1].set_title('Total Loss')\n",
    "    ax[2][0].plot(plt_stats_train['miou'])\n",
    "    ax[2][0].set_title('Mean IoU')\n",
    "    ax[2][1].plot(plt_stats_train['pix_acc'])\n",
    "    ax[2][1].set_title('Pixel Accuracy')\n",
    "    ax[3][0].plot(plt_stats_train['mae'])\n",
    "    ax[3][0].set_title('Mean Absolute Error')\n",
    "    ax[3][1].plot(plt_stats_train['mre'])\n",
    "    ax[3][1].set_title('Mean Absolute Relative Error')\n",
    "    if grad:\n",
    "        ax[4][0].plot(plt_grad)\n",
    "        ax[4][0].set_title('Gradient Norm')\n",
    "    if save:\n",
    "        plt.savefig(f\"./models/{model.name}/{model.name}_train{epochs}.png\")\n",
    "        torch.save(model.state_dict(), f\"./models/{model.name}/{model.name}_train{epochs}.pth\")\n",
    "        # plots = {'plt_losses_train': plt_losses_train, 'plt_losses_val': plt_losses_val, 'plt_stats_train': plt_stats_train, 'plt_stats_val': plt_stats_val}\n",
    "        # save_fig_plots(model, epochs, plots)\n",
    "\n",
    "    if val_dl != None:\n",
    "        _, ax = plt.subplots(3, 1, figsize=(20, 20))\n",
    "        ax[0].plot(plt_losses_val['seg'])\n",
    "        ax[0].set_title('Segmentation Loss')\n",
    "        ax[1].plot(plt_losses_val['depth'])\n",
    "        ax[1].set_title('Depth Loss')\n",
    "        ax[2].plot(plt_losses_val['total'])\n",
    "        ax[2].set_title('Total Loss')\n",
    "        plt.savefig(f\"./models/{model.name}/{model.name}_val{epochs}.png\") if save else None\n",
    "\n",
    "def val_epoch_multitask(model, val_dl, writer, epoch):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_loss_seg = 0\n",
    "        total_loss_depth = 0\n",
    "\n",
    "        miou = MeanIoU(num_classes=model.classes, per_class=False, include_background=False, input_format='index').to(DEVICE)\n",
    "        pix_acc = MulticlassAccuracy(num_classes=model.classes, multidim_average='global', average='micro').to(DEVICE)\n",
    "        stats_seg = {'miou':miou, 'pix_acc':pix_acc}\n",
    "        mae = MeanAbsoluteError().to(DEVICE)\n",
    "        mre = MeanAbsoluteRelativeError().to(DEVICE)\n",
    "        stats_depth = {'mae':mae, 'mre':mre}\n",
    "        for x, y_seg, y_dis in tqdm(val_dl):\n",
    "            loss_seg, loss_depth = compute_loss_multitask(model, x, y_seg, y_dis, stats_seg, stats_depth)\n",
    "            loss = loss_seg + loss_depth\n",
    "            total_loss += loss.item()\n",
    "            total_loss_seg += loss_seg.item()\n",
    "            total_loss_depth += loss_depth.item()\n",
    "        total_loss /= len(val_dl)\n",
    "        total_loss_seg /= len(val_dl)\n",
    "        total_loss_depth /= len(val_dl)\n",
    "        writer.add_scalar('Val/Loss/Total', total_loss, epoch)\n",
    "        writer.add_scalar('Val/Loss/Segmentation', total_loss_seg, epoch)\n",
    "        writer.add_scalar('Val/Loss/Depth', total_loss_depth, epoch)\n",
    "        print(f\"Val Total Loss: {total_loss:.4f}\")\n",
    "        print(f\"Val Loss Segmentation: {total_loss_seg:.4f}\")\n",
    "        print(f\"Val Loss Depth: {total_loss_depth:.4f}\")\n",
    "        losses = {'total': total_loss, 'seg': total_loss_seg, 'depth': total_loss_depth}\n",
    "        # stats_comp = {\n",
    "        #     'miou': stats_seg['miou'].compute().cpu(), \n",
    "        #     'pix_acc': stats_seg['pix_acc'].compute().cpu(), \n",
    "        #     'mae': stats_depth['mae'].compute().cpu(), \n",
    "        #     'mre': stats_depth['mre'].compute().cpu()\n",
    "        # }\n",
    "        stats_comp = dict(stats_seg, **stats_depth)\n",
    "        for k in stats_comp.keys():\n",
    "            print(f\"{k}: {stats_comp[k].compute().cpu()}\")\n",
    "            writer.add_scalar(f'Val/{k}', stats_comp[k].compute().cpu(), epoch)\n",
    "        print(\"\\n\")\n",
    "        return losses, stats_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MTAN()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "nparams = count_params(model)\n",
    "print(f\"Number of trainable parameters: {nparams}\")\n",
    "train_multitask_dwa(model, opt, train_dl, val_dl, epochs=10, update_lambdas=10, T=2, save=True, check=2, grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_singletask(model, x, y, loss_fn, stats):\n",
    "    x = x.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "    output = model(x)\n",
    "    \n",
    "    if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
    "        loss = loss_fn(output, y)\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        preds_flat = preds.view(-1)\n",
    "        y_flat = y.view(-1)\n",
    "        pos_idx = torch.where(y_flat != -1)\n",
    "        preds_flat = preds_flat[pos_idx[0]].unsqueeze(0)\n",
    "        y_flat = y_flat[pos_idx[0]].unsqueeze(0)\n",
    "        update_stats(stats, preds_flat, y_flat)\n",
    "    else:\n",
    "        loss = loss_fn(output.squeeze(1), y)\n",
    "        preds = output.squeeze(1)\n",
    "        update_stats(stats, preds, y)\n",
    "    return loss\n",
    "\n",
    "def train_singletask(model, opt, train_dl, loss_fn, val_dl=None, epochs=10, save=False, check=5, grad=False):\n",
    "    model = model.to(DEVICE)\n",
    "    writer = SummaryWriter(f'./runs/{model.name}')\n",
    "\n",
    "    plt_loss_train = []\n",
    "    plt_grad = []\n",
    "    if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
    "        miou = MeanIoU(num_classes=model.classes, per_class=False, include_background=False, input_format='index').to(DEVICE)\n",
    "        pix_acc = MulticlassAccuracy(num_classes=model.classes, multidim_average='global', average='micro').to(DEVICE)\n",
    "        stats = {'miou':miou, 'pix_acc':pix_acc}\n",
    "    else:\n",
    "        mae = MeanAbsoluteError().to(DEVICE)\n",
    "        mre = MeanAbsoluteRelativeError().to(DEVICE)\n",
    "        stats = {'mae':mae, 'mre':mre}\n",
    "    stats_str = list(stats.keys())\n",
    "    plt_stats_train = {stats_str[0]: [], stats_str[1]: []}\n",
    "    if val_dl != None:\n",
    "        plt_loss_val = []\n",
    "        plt_stats_val = {stats_str[0]: [], stats_str[1]: []}\n",
    "\n",
    "    if save and not os.path.exists(f\"./models/{model.name}\"): \n",
    "        os.makedirs(f\"./models/{model.name}\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        reset_stats(stats)\n",
    "        # for k in stats.keys():\n",
    "        #     stats[k].reset()\n",
    "    \n",
    "        total_loss = 0\n",
    "        for x, y_seg, y_dis in tqdm(train_dl):\n",
    "            x = x.to(torch.float)\n",
    "            y_seg = y_seg.to(torch.long)\n",
    "            y_dis = y_dis.to(torch.float)\n",
    "            y = y_seg.squeeze(dim=1) if isinstance(loss_fn, nn.CrossEntropyLoss) else y_dis\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss = compute_loss_singletask(model, x, y, loss_fn, stats)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        total_loss /= len(train_dl)\n",
    "        writer_string = 'Train/Loss/Segmentation' if isinstance(loss_fn, nn.CrossEntropyLoss) else 'Train/Loss/Depth'\n",
    "        writer.add_scalar(writer_string, total_loss, epoch)\n",
    "        plt_loss_train.append(total_loss)\n",
    "        # for k in stats.keys():\n",
    "        #     plt_stats_train[k].append(stats[k].compute().cpu())\n",
    "        add_plt(plt_stats_train, stats)\n",
    "        if grad:\n",
    "            grad_norm = compute_grad(model)\n",
    "            plt_grad.append(grad_norm)\n",
    "            writer.add_scalar('Train/Gradient', grad_norm, epoch)\n",
    "        if epoch % check == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} - Train Loss: {total_loss:.4f}\")\n",
    "            for k in stats.keys():\n",
    "                print(f\"{k}: {stats[k].compute().cpu()}\")\n",
    "            \n",
    "            print(f\"Gradient Norm: {grad_norm}\\n\")\n",
    "            save_model_opt(model, opt, epoch) if save else None\n",
    "                                \n",
    "        if val_dl != None and epoch % check == 0:\n",
    "            losses_tmp, stats_tmp = val_epoch_singletask(model, val_dl, loss_fn, writer, epoch)\n",
    "            plt_loss_val.append(losses_tmp)\n",
    "            for k in stats_tmp.keys():\n",
    "                plt_stats_val[k].append(stats_tmp[k].compute().cpu())\n",
    "\n",
    "    _, ax = plt.subplots(2, 2, figsize=(40, 40))\n",
    "    ax[0][0].plot(plt_loss_train)\n",
    "    ax[0][0].set_title('Loss')\n",
    "    ax[0][1].plot(plt_stats_train[stats_str[0]])\n",
    "    ax[0][1].set_title(stats_str[0])\n",
    "    ax[1][0].plot(plt_stats_train[stats_str[1]])\n",
    "    ax[1][0].set_title(stats_str[1])\n",
    "    if grad:\n",
    "        ax[1][1].plot(plt_grad)\n",
    "        ax[1][1].set_title('Gradient Norm')\n",
    "    if save:\n",
    "        plt.savefig(f\"./models/{model.name}/{model.name}_train{epochs}.png\")\n",
    "        torch.save(model.state_dict(), f\"./models/{model.name}/{model.name}_train{epochs}.pth\")\n",
    "        # plots = {'plt_loss_train': plt_loss_train, 'plt_stats_train': plt_stats_train, 'plt_loss_val': plt_loss_val, 'plt_stats_val': plt_stats_val}\n",
    "        # save_fig_plots(model, epochs, plots)\n",
    "\n",
    "    if val_dl != None:\n",
    "        _, ax = plt.subplots(3, 1, figsize=(20, 20))\n",
    "        ax[0].plot(plt_loss_val)\n",
    "        ax[0].set_title('Loss')\n",
    "        for i, k in enumerate(stats.keys()):\n",
    "            ax[i+1].plot(plt_stats_val[k])\n",
    "            ax[i+1].set_title(k)\n",
    "        plt.savefig(f\"./models/{model.name}_val{epochs}.png\") if save else None\n",
    "\n",
    "\n",
    "def val_epoch_singletask(model, dl, loss_fn, writer, epoch):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
    "            miou = MeanIoU(num_classes=model.classes, per_class=False, include_background=False, input_format='index').to(DEVICE)\n",
    "            pix_acc = MulticlassAccuracy(num_classes=model.classes, multidim_average='global', average='micro').to(DEVICE)\n",
    "            stats = {'miou': miou, 'pix_acc': pix_acc}\n",
    "        else:\n",
    "            mae = MeanAbsoluteError().to(DEVICE)\n",
    "            mre = MeanAbsoluteRelativeError().to(DEVICE)\n",
    "            stats = {'mae': mae, 'mre': mre}\n",
    "        for x, y_seg, y_dis in tqdm(dl):\n",
    "            x = x.to(torch.float)\n",
    "            y_seg = y_seg.to(torch.long)\n",
    "            y_dis = y_dis.to(torch.float)\n",
    "            y = y_seg.squeeze(dim=1) if isinstance(loss_fn, nn.CrossEntropyLoss) else y_dis\n",
    "            loss = compute_loss_singletask(model, x, y, loss_fn, stats)\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "        total_loss /= len(dl)\n",
    "        writer_string = 'Test/Loss/Segmentation' if isinstance(loss_fn, nn.CrossEntropyLoss) else 'Train/Loss/Depth'\n",
    "        writer.add_scalar(writer_string, total_loss, epoch)\n",
    "        print(\"Test Loss: \", total_loss)\n",
    "        for k in stats.keys():\n",
    "            print(f\"{k}: {stats[k].compute().cpu()}\")\n",
    "        print(\"\\n\")\n",
    "    return total_loss, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segnet = SegNet()\n",
    "opt = torch.optim.Adam(segnet.parameters(), lr=LEARNING_RATE)\n",
    "# print(densenet)\n",
    "nparams = sum(p.numel() for p in segnet.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {nparams}\")\n",
    "train_singletask(segnet, opt, train_dl, nn.CrossEntropyLoss(ignore_index=-1), val_dl, epochs=EPOCHS, save=True, check=5, grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results_multitask(model, img, img_seg, img_dis, save=False):\n",
    "    with torch.no_grad():\n",
    "        model = model.to(DEVICE)\n",
    "        model.eval()\n",
    "        img = img.to(DEVICE).to(torch.float)\n",
    "        output_seg, output_dis = model(img.unsqueeze(0))\n",
    "        pred_seg = torch.argmax(output_seg, dim=1).squeeze(0).cpu().detach().numpy()\n",
    "        pred_dis = output_dis.squeeze(0, 1).cpu().detach().numpy()\n",
    "        idx = img_seg==-1\n",
    "        img_seg[idx] = 0\n",
    "        print(f\"Accuracy: {torch.sum(pred_seg == img_seg) / (img_seg.numel()-torch.sum(idx))}\")\n",
    "\n",
    "        plt.imshow(img.cpu().permute(1, 2, 0))\n",
    "\n",
    "        _, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "        ax[0][0].imshow(img_seg)\n",
    "        ax[0][0].set_title('Ground Truth Segmentation')\n",
    "        ax[0][1].imshow(pred_seg)\n",
    "        ax[0][1].set_title('Predicted Segmentation')\n",
    "        ax[1][0].imshow(img_dis, cmap='gray')\n",
    "        ax[1][0].set_title('Ground Truth Depth')\n",
    "        ax[1][1].imshow(pred_dis, cmap='gray')\n",
    "        ax[1][1].set_title('Predicted Depth')\n",
    "        plt.show()\n",
    "        if save:\n",
    "            if not os.path.exists(f\"./models/{model.name}\"): \n",
    "                os.makedirs(f\"./models/{model.name}\")\n",
    "            plt.savefig(f\"./models/{model.name}/{model.name}_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (img, img_seg, img_dis) in enumerate(val_dl):\n",
    "#     visualize_results(model, img[1], img_seg[1], img_dis[1])\n",
    "#     if i == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import colors\n",
    "\n",
    "# cmap = colors.ListedColormap(['k','b','y','g','r'])\n",
    "# rm = np.random.randint(0,5,(5,5))\n",
    "# print(rm)\n",
    "# plt.imshow(rm, interpolation='nearest', cmap=cmap)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
